{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7826dd97-2785-45ef-9772-77d4e99bc8c5",
   "metadata": {},
   "source": [
    "# Testes Funções \"Caseiras\"\n",
    "\n",
    "1. Regressão logística Simples usando função de otimização;\n",
    "2. Regressão logística Simples usando o algorítmo de Gradient Descent;\n",
    "3. Regressão logistica multinomial/multiclasse Softmax usando função de otimização;\n",
    "4. Regressão logistica multinomial/multiclasse Softmax usando o algoritmo Gradient Descent;\n",
    "5. Funçoes de Classificação One vs One e One vs Rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee118e-8037-45c7-9325-c9629ec51758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Importando Dados e tratando.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "col_names = [ \"sepal length in cm\",\n",
    "              \"sepal width in cm\",\n",
    "              \"petal length in cm\",\n",
    "              \"petal width in cm\",\n",
    "              \"class\" ]\n",
    "\n",
    "iris_ds = pd.read_csv( r\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "                       sep = \",\", names = col_names )\n",
    "\n",
    "classes   = np.unique( iris_ds['class'] )\n",
    "\n",
    "dic_class = { classes[k] : k for k in range(0,len( classes ) ) }\n",
    "\n",
    "def class_to_n( k ):    \n",
    "    if k == \"Iris-setosa\":\n",
    "        return 0\n",
    "    if k == \"Iris-versicolor\":\n",
    "        return 1\n",
    "    if k == \"Iris-virginica\":\n",
    "        return 2\n",
    "\n",
    "iris_alt = ( iris_ds. \n",
    "               assign( y0   = lambda d: [ 1 if i == 'Iris-setosa'     else 0 for i in iris_ds['class']],\n",
    "                       y1   = lambda d: [ 1 if i == 'Iris-versicolor' else 0 for i in iris_ds['class']],\n",
    "                       y2   = lambda d: [ 1 if i == 'Iris-virginica'  else 0 for i in iris_ds['class']],\n",
    "                     \n",
    "                       class_2 = lambda x: list( map( lambda k: dic_class[ k ], x[\"class\"] ) )) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38820324-a00a-488c-ac8f-7443f56301d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "names = [ \"sepal length in cm\", \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\" ]\n",
    "\n",
    "comb = list(itertools.combinations( names , 2) )\n",
    "\n",
    "fig, ax = plt.subplots( 2,3, figsize=(16,9), sharey=True )\n",
    "\n",
    "cords    = [ [j,i]  for j in range( 0, 2 )  for i in range( 0,  3 ) ]\n",
    "\n",
    "# print( cords )\n",
    "\n",
    "for i in range( 0, len( comb ) ):\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        ax = ax[ cords[i][0], cords[i][1] ],\n",
    "        data = iris_alt,\n",
    "        x = comb[i][0], y = comb[i][1], hue = 'class'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ba261-5d95-4498-b36f-90ae9ed5c879",
   "metadata": {},
   "source": [
    "# 1. Regressão logística Simples usando função de otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4798f5c-530c-40f9-a1bc-bfa27c11bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "os.chdir( dir_path.replace( \"notebooks\", \"src\" ) )\n",
    "\n",
    "%run Logistic_Simples_Multinomial.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88999161-2922-4e15-bc8f-5b2f35b8a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_logreg = Logistic_Reg( X = np.array( iris_alt.loc[:,['petal width in cm']] ), y = np.array( iris_alt.loc[:,['y0']] ), int_ = False )\n",
    "        \n",
    "model_2 = model_logreg.fit( reg = \"l2\" )\n",
    "\n",
    "model_2 = model_2.predict( Xp =  np.array( iris_alt.loc[:,['petal width in cm']] ) )\n",
    "\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "\n",
    "#x.plot(  np.array( data_par[ data_par.columns[ i ] ] ) )\n",
    "\n",
    "\n",
    "ax.plot( list( map( lambda x: x[0], model_2.model[\"par_hist\"] ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da05d8-1fe8-4c95-abb1-19a0158bfd35",
   "metadata": {},
   "source": [
    "# 2. Regressão logística Simples usando o algorítmo de Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a53b0-599b-48ef-a8d7-ad3fc8cd94a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56caf57-1472-46c9-bd5c-4f62ff908784",
   "metadata": {},
   "source": [
    "# 3. Regressão logistica multinomial/multiclasse Softmax usando função de otimização\n",
    "\n",
    "Uma breve introdução a regressão Logística Softmax:\n",
    "\n",
    "# Regressão Logística Multinomial Softmax\n",
    "\n",
    "Na seção acima, foi apresentado toda a problemática envolvendo as regressões logísticas, aonde desejavamos classificar observações entre acontecimentos e não acontecimentos de um determinado evento, porém para além de problemas de classificação binária, é comum se encontrar em situações aonde desejamos classificar um grupo de observações em contextos que envolvem mais de uma classe, podendo essa variar de 2 para muitas classes.\n",
    "\n",
    "É nesse contexto que se encaixam a regressão logística multinomial softmax, ao invés de classificar uma saida binaria generalizamos a regressão logística para N classes.\n",
    "\n",
    "$$ \\begin{bmatrix} P( y = 1 | \\theta ) \\\\ P( y = 2 | \\theta ) \\\\ ... \\\\ P( y = N | \\theta ) \\end{bmatrix} = \\frac{1}{\\sum_{j=1}^{K}exp( \\theta_{j}^{T}X )} \\begin{bmatrix} exp( \\theta_{1}^{T}X ) \\\\ exp( \\theta_{2}^{T}X ) \\\\ ... \\\\ exp( \\theta_{N}^{T}X ) \\end{bmatrix} $$\n",
    "\n",
    "aonde:\n",
    "\n",
    "$$ \\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ ... \\\\ \\theta_N \\end{bmatrix} $$\n",
    "\n",
    "Podemos passar de uma notação vetorial para uma equação mais geral, aonde temos a função custo da regressão softmax:\n",
    "\n",
    "$$ P(y_{i} = k | \\theta ) = \\frac{ exp( \\theta_{k}^{T}X ) }{ \\sum_{ j \\in K }exp( \\theta_{j}^{T}X ) } $$\n",
    "\n",
    "Para a estimação por meio de algoritmo, como \\textit{ gradient descent }, remos abaixo um exemplo dos gradientes calculados:\n",
    "\n",
    "$$ \\nabla f(\\theta) = - \\sum_{i=1}^{m} [1*( j = i ) - P(y_{i} = k | \\theta )]  $$\n",
    "\n",
    "\n",
    "## Softmax caso de duas classes\n",
    "\n",
    "Caso especial quando estimamos a regressão logística multinomial Softmax para um número de classes igual a dois conseguimos fazer uma relação entre a regressao softmax e uma logistica de saida binaria. Assim\n",
    "\n",
    "$$ \\begin{bmatrix} P( y = 1 | \\theta ) \\\\ P( y = 2 | \\theta ) \\end{bmatrix} = \\frac{1}{ exp( \\theta_{1}^{T}X ) + exp( \\theta_{2}^{T}X )} \\begin{bmatrix} exp( \\theta_{1}^{T}X ) \\\\ exp( \\theta_{2}^{T}X ) \\end{bmatrix} $$\n",
    "\n",
    "é possivel manipular algebricamente essa equação de modo que se subtraia $\\theta_2$ das equaçoes de modo que a equação fique:\n",
    "\n",
    "$$ \\begin{bmatrix} P( y = 1 | \\theta ) \\\\ P( y = 2 | \\theta ) \\end{bmatrix} = \\frac{1}{ exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) + exp( (\\theta_{2}^{T} - \\theta_{2}^{T})X )} \\begin{bmatrix} exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) \\\\ exp( (\\theta_{2}^{T} - \\theta_{2}^{T})X ) \\end{bmatrix} $$\n",
    "\n",
    "$$ \\begin{bmatrix} P( y = 1 | \\theta ) \\\\ P( y = 2 | \\theta ) \\end{bmatrix} = \\frac{1}{ exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) + exp( 0 )} \\begin{bmatrix} exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) \\\\ exp( 0 ) \\end{bmatrix} $$\n",
    "\n",
    "$$ \\begin{bmatrix} P( y = 1 | \\theta ) \\\\ P( y = 2 | \\theta ) \\end{bmatrix} = \\frac{1}{ 1 + exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) } \\begin{bmatrix} exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) \\\\ 1 \\end{bmatrix} $$\n",
    "\n",
    "Retirando da notação vetorial temos a seguinte forma:\n",
    "\n",
    "$$ P( y = 1 | \\theta ) = \\frac{ exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X )  }{ 1 + exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) } $$\n",
    "\n",
    "$$ P( y = 2 | \\theta ) = \\frac{ 1 }{ 1 + exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) } $$\n",
    "\n",
    "Manipulando algebricamente podemos chegar a conclusão que no segundo caso:\n",
    "\n",
    "$$ P( y = 2 | \\theta ) = 1 - P( y = 1 | \\theta ) = 1 - \\frac{ exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) }{ 1 + exp( (\\theta_{1}^{T} - \\theta_{2}^{T})X ) } $$\n",
    "\n",
    "Logo chegamos ao estágio em que encontramos uma relação direta entre a regressão logistica multinomial softmax e a regressão logística, na regressão logistica softmax encontramos uma curva que discrimina cada classe índividualmente das demais, enquanto no caso da regressão logística, encontramos apenas uma curva que busca discriminar as duas classes, ao estimar uma regressão softmax para o caso de N = 2, são calculadas duas curvas, porém a subtração dos parametros seria o resultado de uma curva calculada por um modelo de regressão logística convencional.\n",
    "\n",
    "\n",
    "Referência: http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b8208-572a-427a-8d66-b93ea26d6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = iris_alt[['petal length in cm','petal width in cm']], iris_alt['class_2']\n",
    "\n",
    "clf = LogisticRegression(random_state=0, fit_intercept = True, penalty = 'none').fit(X, y )\n",
    "# Função para plotar Funções\n",
    "\n",
    "print( clf.intercept_ )\n",
    "print( clf.coef_ )\n",
    "\n",
    "plot_softmax_model( model_ = clf, X = X, y = y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d84736-09fa-4212-8c1a-041c4c20c946",
   "metadata": {},
   "source": [
    "# 4. Regressão logistica multinomial/multiclasse Softmax usando o algoritmo Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6062d84-65f1-4c0d-98fb-984a2005e063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e19426f-709c-4258-b287-cc29b7a7c7e3",
   "metadata": {},
   "source": [
    "# 5. Funçoes de Classificação One vs One e One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59f772-9206-42bb-b36b-5d734a24530c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
